{"cells":[{"cell_type":"code","execution_count":null,"id":"9dc2dc51-71fe-4bf1-b04a-b2f495d7f063","metadata":{"id":"9dc2dc51-71fe-4bf1-b04a-b2f495d7f063"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"9109c8d7-96f6-4b06-b1e7-3a6c2c9047c9","metadata":{"id":"9109c8d7-96f6-4b06-b1e7-3a6c2c9047c9"},"source":["## Redes Convolutivas CNN\n","\n","Las **capas convolutivas** son los bloques de construcción de las CNN. En estas redes, las neuronas en la capa de entrada no están conectadas a cada pixel de la imagen. Sólo se conectan a los pixeles que están dentro de su **campo de recepción** (dado por el tamaño de **parche** o **kernel**).\n","\n","Lo mismo se cumple para las capas siguientes, esta característica le permite a la red concentrarse en características de bajo nivel en la primera capa, luego unirlas en características de más alto nivel en las capas siguientes y así sucesivamente.\n","\n","Entonces cada capa está realmente representada en 2D, lo cual hace más simple emparejar las neuronas con sus entradas correspondientes."]},{"cell_type":"markdown","id":"d1eb0f90-f56a-4772-bed7-c64dca9db980","metadata":{"id":"d1eb0f90-f56a-4772-bed7-c64dca9db980"},"source":["**kernel:** Los pesos de las neuronas pueden ser representados como una imagen pequeña que tiene el tamaño del campo de recepción. Una capa a la cual se le aplica un filtro, devuelve como salida un **mapa de características** que resalta las áreas de la imagen que activan al máximo el kernel o flitro.\n","\n","Estos filtros no se definen manualmente. Durante el entrenamiento la capa de convolución aprende automáticamente cuáles son los filtros más útilies para su tarea y las capas siguientes aprenden como combinarlos en patrones más complejos."]},{"cell_type":"markdown","id":"13e9c07b-8a2c-46bf-9567-47e2e5fb4e9b","metadata":{"id":"13e9c07b-8a2c-46bf-9567-47e2e5fb4e9b"},"source":["**Padding:** Para que cada capa tenga las mismas dimensiones que la capa anterior, es común agregar ceros en la capa de entrada _zero padding_\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1l1KzjvY_OVbipRP7IYy7-YOI0pt-A44c\" width= 500 alt=\"centered image\"></center>"]},{"cell_type":"markdown","id":"20ae0ca3-ae7a-4844-ae28-071407a54bd4","metadata":{"id":"20ae0ca3-ae7a-4844-ae28-071407a54bd4"},"source":["**Stride:** También es posible conectar una capa de entrada más grande a una más pequeña, espaciando los campos de recepción. Esto reduce la complejidad computacional del modelo. El espaciamiento entre una campo de recepción y el siguiente, se denomina stride\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1l3gqDQ1vlxNEqt90_iyYPTMVuDbcwu92\" width= 500 alt=\"centered image\"></center>"]},{"cell_type":"markdown","id":"d29fa837-3ab4-4a02-b222-ffb34daf4812","metadata":{"id":"d29fa837-3ab4-4a02-b222-ffb34daf4812"},"source":["**Apilamiento de mapas de características** En la práctica una capa de convolución tiene más de 1 kernel, por tanto se obtienen mapas de características, uno por kernel, esto es mejor representado en 3D:\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1l4v27c9rpvhPB7C2ZItGP06MlBaS5zzW\" width= 500 alt=\"centered image\"></center>"]},{"cell_type":"markdown","id":"02eac3d2-7e4b-4291-9ac7-e0f9fb90c5cf","metadata":{"id":"02eac3d2-7e4b-4291-9ac7-e0f9fb90c5cf"},"source":["### Ejemplo de convolución\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a053fb65-32bb-4fd6-a998-8ae591e1b022","metadata":{"id":"a053fb65-32bb-4fd6-a998-8ae591e1b022"},"outputs":[],"source":["def plot_image(image):\n","    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n","    plt.axis(\"off\")\n","\n","def plot_color_image(image):\n","    plt.imshow(image, interpolation=\"nearest\")\n","    plt.axis(\"off\")"]},{"cell_type":"code","execution_count":null,"id":"cc9fc298-cb7c-4990-894b-e7ea3a4e8d70","metadata":{"id":"cc9fc298-cb7c-4990-894b-e7ea3a4e8d70"},"outputs":[],"source":["from sklearn.datasets import load_sample_image\n","\n","# cargo imágenes de muestra y las escalo entre 0-1\n","china = load_sample_image(\"china.jpg\") / 255\n","flower = load_sample_image(\"flower.jpg\") / 255\n","plot_color_image(china)\n","print(china.shape)"]},{"cell_type":"code","execution_count":null,"id":"2c590670","metadata":{"id":"2c590670"},"outputs":[],"source":["plot_color_image(flower)\n","print(flower.shape)"]},{"cell_type":"code","execution_count":null,"id":"a67e1b1c-a756-4530-b88b-56a5d3d570d0","metadata":{"id":"a67e1b1c-a756-4530-b88b-56a5d3d570d0"},"outputs":[],"source":["# mini-batch de 2 imágenes\n","imagenes = np.array([china, flower])\n","batch_size, height, width, canales = imagenes.shape\n","print(\"Dimensión de entrada: \", imagenes.shape)\n","\n","# Creo 2 filtros de 7x7\n","filtros = np.zeros(shape=(7, 7, canales, 2), dtype=np.float32)\n","print(\"Dimensión de los kernels: \", filtros.shape)\n","filtros[:, 3, :, 0] = 1  # linea vertical blanca\n","filtros[3, :, :, 1] = 1  # linea horizontal blanca\n","\n","outputs = tf.nn.conv2d(imagenes, filtros, strides=1, padding=\"SAME\")\n","# como tenemos 2 filtros\n","# para cada imagen de entrada, obtengo 2 mapas de características\n","# nro de imagenes, alto, ancho, nro de mapas x imagen\n","\n","print(\"Dimensión de salida: \", outputs.shape)\n","\n","print(filtros[:,:,0,0])\n","print(filtros[:,:,0,1])\n","print(filtros[:,:,2,1])"]},{"cell_type":"code","execution_count":null,"id":"a9f67df1-27f3-4d77-b9e2-bb6037c77345","metadata":{"tags":[],"id":"a9f67df1-27f3-4d77-b9e2-bb6037c77345"},"outputs":[],"source":["# grafico el primer mapa de características para la 1era imagen\n","# imagen con filtro de linea vertical blanca\n","# recorto la imagen [150:220, 130:250]\n","plt.imshow(outputs[0, :, :, 0][150:220, 130:250], cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"a3aa9210-ae18-4f82-bfef-4b14774c7820","metadata":{"tags":[],"id":"a3aa9210-ae18-4f82-bfef-4b14774c7820"},"outputs":[],"source":["# grafico el segundo mapa de características para la 1era imagen\n","# imagen con filtro de linea horizontal blanca\n","plt.imshow(outputs[0, :, :, 1][150:220, 130:250], cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","id":"b03a3203-8ad0-4b93-8232-de37b412d020","metadata":{"id":"b03a3203-8ad0-4b93-8232-de37b412d020"},"source":["## Usando una capa de convolución\n","Creamos una capa de convolución 2D usando **keras.layers.Conv2D()**"]},{"cell_type":"code","execution_count":null,"id":"fff13cd2-f6ce-4e8c-b32b-93a75cec2535","metadata":{"id":"fff13cd2-f6ce-4e8c-b32b-93a75cec2535"},"outputs":[],"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","conv = keras.layers.Conv2D(\n","                            filters=2,\n","                            kernel_size=7,\n","                            strides=1,\n","                            padding=\"SAME\",\n","                            activation=\"relu\",\n","                            input_shape= [427, 640, 3]\n","                          )"]},{"cell_type":"markdown","id":"79e60b4f-9a09-431c-8741-b921fa18b3e8","metadata":{"id":"79e60b4f-9a09-431c-8741-b921fa18b3e8"},"source":["Aplicamos esta capa a las imágenes"]},{"cell_type":"code","execution_count":null,"id":"0a77dbd3-9874-492f-8c1a-11beeecd0e36","metadata":{"id":"0a77dbd3-9874-492f-8c1a-11beeecd0e36"},"outputs":[],"source":["conv_outputs = conv(imagenes)\n","conv_outputs.shape"]},{"cell_type":"markdown","id":"61de4179-310d-4b21-a51b-4a1a31e3608f","metadata":{"id":"61de4179-310d-4b21-a51b-4a1a31e3608f"},"source":["La salida es un tensor 4D. Las dimensiones son: tamaño de batch, alto, ancho, nro de mapas de características. La primera dimensión (tamaño del batch) porque tenemos 2 imágenes de entrada. Las siguientes dos dimensiones son la altura y ancho de los mapas de características de salida: dado que **padding=\"SAME\"** y **strides=1**, los mapas de características de salida tienen las mismas dimensiones que las imágenes de entrada (en este caso 427×640). Esta capa convolucional tiene 2 filtros, por esto, la última dimensión es 2: hay 2 mapas de características de salida por cada imagen de entrada.\n","\n","Para determinar cada dimensión del mapa de características de salida se puede utilizar la siguiente fórmula:\n","\n","$$o={\\frac{n+2p-m}{s}} + 1$$\n","\n","asumiendo que $n$ es una de las dimensiones de la imagen de entrada, $m$ la dimensión del kernel, $p$ el valor de padding y $s$ el stride"]},{"cell_type":"markdown","id":"0318891f","metadata":{"id":"0318891f"},"source":["Los filtros de la función Conv2D se inicializan aleatoriamente, por ende, se detectarán patrones aleatorios. Echemos un vistazo a los 2 mapas de características de salida para cada imagen:"]},{"cell_type":"code","execution_count":null,"id":"e0376cce-ab1c-4471-855f-12aba674a6ce","metadata":{"id":"e0376cce-ab1c-4471-855f-12aba674a6ce"},"outputs":[],"source":["plt.figure(figsize=(12,7))\n","for indice in (0, 1):\n","    for indice_mapa in (0, 1):\n","        plt.subplot(2, 2, indice * 2 + indice_mapa + 1)\n","        plot_image(conv_outputs[indice, :, :, indice_mapa][150:220, 130:250])\n","plt.show()"]},{"cell_type":"markdown","id":"9d9c1772-154e-41c1-a304-3da25681d0c4","metadata":{"id":"9d9c1772-154e-41c1-a304-3da25681d0c4"},"source":["Aunque los filtros se inicializaron aleatoriamente, el segundo filtro actúa como un detector de bordes. Los filtros inicializados aleatoriamente a menudo actúan de esta manera, lo cual está bueno ya que la detección de bordes es bastante útil en el procesamiento de imágenes.\n","\n","Si queremos, podemos configurar los filtros para que sean como los definidos anteriormente en cero. Casi nunca necesitaremos configurar filtros manualmente, ya que la capa convolucional solo aprenderá los filtros adecuados durante el entrenamiento"]},{"cell_type":"code","execution_count":null,"id":"14d95fc2-826d-4941-b259-073ca4697ce9","metadata":{"id":"14d95fc2-826d-4941-b259-073ca4697ce9"},"outputs":[],"source":["conv.set_weights([filtros, np.zeros(2)])"]},{"cell_type":"code","execution_count":null,"id":"91f6242d-f83f-4787-ac79-c49c2400db92","metadata":{"id":"91f6242d-f83f-4787-ac79-c49c2400db92"},"outputs":[],"source":["conv_outputs = conv(imagenes)\n","\n","plt.figure(figsize=(10,6))\n","for indice in (0, 1):\n","    for indice_mapa in (0, 1):\n","        plt.subplot(2, 2, indice * 2 + indice_mapa + 1)\n","        plot_image(conv_outputs[indice, :, :, indice_mapa][150:220, 130:250])\n","plt.show()"]},{"cell_type":"markdown","id":"e54b393f-bf04-4aa9-8cba-e96fa4e16a22","metadata":{"id":"e54b393f-bf04-4aa9-8cba-e96fa4e16a22"},"source":["### Pooling\n","\n","Otro tipo de capa presente en las CNN, su objetivo es “submuestrear” la imagen de entrada para reducir la carga computacional, uso de memoria y disminuir número de parámetros, por tanto disminuye el riesgo de overfitting.\n","Al igual que con las capas de convolución, cada neurona en la capa pooling está conectada a las salidas de un número limitado de neuronas de la capa anterior dentro del campo de recepción. También se debe definir su tamaño, el stride y el tipo de padding. Sin embargo las neuronas de la capa de pooling no tienes pesos"]},{"cell_type":"markdown","id":"0753edc4-15a1-4572-9988-0ade40f683a3","metadata":{"id":"0753edc4-15a1-4572-9988-0ade40f683a3"},"source":["**Max pooling:** En este caso, sólo el valor máximo del campo de recepción pasa a la siguiente capa. En general se aplican a cada canal de entrada de forma independiente.\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1kYdnLDATkI4daQvR9ivxXi7tS-jJ6lxi\" width= 500 alt=\"centered image\"></center>\n","A continuación se crea una capa de Max pooling con kernel 2x2"]},{"cell_type":"code","execution_count":null,"id":"3a81f4bd-fb34-4b8a-8463-215a7239e06c","metadata":{"id":"3a81f4bd-fb34-4b8a-8463-215a7239e06c"},"outputs":[],"source":["max_pool = keras.layers.MaxPool2D(pool_size=2)\n","#pool_size: tamaño del kernel\n","# por defecto stride = tamaño del kernel, padding='valid'"]},{"cell_type":"code","execution_count":null,"id":"fac70d2e-c8e5-499e-abd0-c12808f704ab","metadata":{"id":"fac70d2e-c8e5-499e-abd0-c12808f704ab"},"outputs":[],"source":["img_recortadas = np.array([imagen[150:220, 130:250] for imagen in imagenes], dtype=np.float32)\n","img_recortadas.shape"]},{"cell_type":"code","execution_count":null,"id":"6478a744-340e-4590-842c-f100e15718a0","metadata":{"tags":[],"id":"6478a744-340e-4590-842c-f100e15718a0"},"outputs":[],"source":["plot_color_image(img_recortadas[0])"]},{"cell_type":"markdown","id":"2f6232b4-8d3b-4567-bec7-bd60aa6af19b","metadata":{"id":"2f6232b4-8d3b-4567-bec7-bd60aa6af19b"},"source":["Aplicamos a las imágenes recortadas la capa de Max pooling"]},{"cell_type":"code","execution_count":null,"id":"4145c8c1-557d-4c2d-89f4-1e1cfe858367","metadata":{"id":"4145c8c1-557d-4c2d-89f4-1e1cfe858367"},"outputs":[],"source":["output_max = max_pool(img_recortadas)\n","output_max.shape"]},{"cell_type":"code","execution_count":null,"id":"cf8f0316-476a-4ffc-bac3-33b96406e721","metadata":{"id":"cf8f0316-476a-4ffc-bac3-33b96406e721"},"outputs":[],"source":["fig = plt.figure(figsize=(12, 8))\n","gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n","\n","ax1 = fig.add_subplot(gs[0, 0])\n","ax1.set_title(\"Original\", fontsize=14)\n","ax1.imshow(img_recortadas[0])\n","ax1.axis(\"off\")\n","ax2 = fig.add_subplot(gs[0, 1])\n","ax2.set_title(\"con Max pooling\", fontsize=14)\n","ax2.imshow(output_max[0])\n","ax2.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","id":"61322768-c5b2-45be-b563-d21bff29d64d","metadata":{"id":"61322768-c5b2-45be-b563-d21bff29d64d"},"source":["**Average pooling:** calcula el promedio de los valores en el campo de recepción"]},{"cell_type":"code","execution_count":null,"id":"4d444eb6-3384-429f-b753-b2990b56ec45","metadata":{"tags":[],"id":"4d444eb6-3384-429f-b753-b2990b56ec45"},"outputs":[],"source":["avg_pool = keras.layers.AvgPool2D(pool_size=2)"]},{"cell_type":"code","execution_count":null,"id":"e7426a60-307a-4f0c-859a-6aa3de835a42","metadata":{"tags":[],"id":"e7426a60-307a-4f0c-859a-6aa3de835a42"},"outputs":[],"source":["output_avg = avg_pool(img_recortadas)\n","output_avg.shape"]},{"cell_type":"code","execution_count":null,"id":"64bb99cb-a9fb-4e93-8b2f-bcdc0846d023","metadata":{"tags":[],"id":"64bb99cb-a9fb-4e93-8b2f-bcdc0846d023"},"outputs":[],"source":["fig = plt.figure(figsize=(12, 8))\n","gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n","\n","ax1 = fig.add_subplot(gs[0, 0])\n","ax1.set_title(\"Input\", fontsize=14)\n","ax1.imshow(img_recortadas[0])  # plot the 1st image\n","ax1.axis(\"off\")\n","ax2 = fig.add_subplot(gs[0, 1])\n","ax2.set_title(\"Output\", fontsize=14)\n","ax2.imshow(output_avg[0])  # plot the output for the 1st image\n","ax2.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","id":"188390f9-bddb-4b32-8747-919b8c505783","metadata":{"id":"188390f9-bddb-4b32-8747-919b8c505783"},"source":["**Global Average Pooling:** calcula el promedio pero con todo el mapa de características de entrada"]},{"cell_type":"code","execution_count":null,"id":"cbfc2eca-f877-4eb5-848f-94da4e438820","metadata":{"id":"cbfc2eca-f877-4eb5-848f-94da4e438820"},"outputs":[],"source":["global_avg_pool = keras.layers.GlobalAvgPool2D()"]},{"cell_type":"code","execution_count":null,"id":"2fcd139f-44a7-49e3-9e64-5ee370e8d2d0","metadata":{"id":"2fcd139f-44a7-49e3-9e64-5ee370e8d2d0"},"outputs":[],"source":["output_g_avg = global_avg_pool(img_recortadas)\n","output_g_avg #obtenemos la intensidad media de los 3 canales RGB para cada imagen"]},{"cell_type":"markdown","id":"f7415dda-ae10-428d-871a-b311d8f914fd","metadata":{"id":"f7415dda-ae10-428d-871a-b311d8f914fd"},"source":["**Dropout** es una de las técnicas de regularización más populares en deep learning. Es un algoritmo bastante simple: en cada paso de entrenamiento, cada neurona (incluidas las neuronas de entrada, pero siempre excluyendo las neuronas de salida) tienen una probabilidad $p$ de ser temporalmente \"desconectadas\", lo que significa que se ignorará por completo durante esa iteración de entrenamiento, pero puede estar activa en la siguiente. El hiperparámetro $p$ se denomina **dropout rate** y, por lo general, se establece entre el 10% y el 50 %: más cerca de 20–30% en redes neuronales recurrentes, y más cerca del 40-50% en redes convolucionales. Después del entrenamiento, las neuronas no se descontectan más."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}